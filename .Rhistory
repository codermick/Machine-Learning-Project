x
x[c(TRUE)] <- 1
x
L <- list(x = c(1:5), y = c("a", "b", "c"), z = capColor)
L
L$y
L[[2]]
L[c(1, 3)]
L[c("x","y")]
round((0.75*0.3)/(  (0.75*0.3) +  ((1-0.52)*(1-0.3))  )*100)
x <- 1:4
p <- x/sum(x)
temp <- rbind(x, p)
rownames(temp) <- c("X", "Prob")
temp
pi
2+3
sqrt(2)
exp(1)
exp(3)
x <- 7*41/pi
x
x <- c(74,31,95,61,34,23,54,96)
x
seq(from = 1, to = 5)
seq(1:5)
x[1]
x[2:4]
x(c[1,2,8])
x[c(1,2,8)]
letters[1:5]
LETTERS[1:10]
x <- 1:5
sum(x)
length(x)
min(x)
mean(x)
sd(x)
sd
intersect
rev
methods(rev)
apropos
str(precip)
precip[1:4]
str
str(rivers)
str(discoveries)
names(discoveries)
head(discoveries)
plot(LakeHuron, type = "h")
plot
plot(LakeHuron, type = "p")
tbl <- table(state.division)
tbl
tbl/sum(tbl)
prop.table(tbl)
barplot(table(state.region) cex.names = 0.5)
barplot(table(state.region), cex.names = 0.5)
barplot(prop.table(table(state.region)), cex.names = 0.5)
library(qcc)
pareto.chart
pareto.chart(table.division), ylab = "Frequency")
library()
install.packages("RcmdrPlugin.IPSUR" , depends = TRUE)
install.packages("RcmdrPlugin.IPSUR")
pareto.chart(table(state.division), ylab = "Frequency")
library(qcc)
library("Rcmdr", lib.loc="~/R/win-library/3.1")
library("RcmdrPlugin.IPSUR", lib.loc="~/R/win-library/3.1")
Rcmdr
Library("Rcmdr")
library(RcmdrPlugin.IPSUR)
data(RcmdrTestDrive)
attach(RcmdrTestDrive)
names(RcmdrTestDrive)
View(RcmdrTestDrive)
library("Rcmdr")
open("Rcmdr")
launch("Rcmdr")
x <- table(iris)
x
mean(x)
median(x)
?quantile
pnorm(0.7)
prnorm(-0.7)
pnorm(-0.7)
qnorm(0.95)
qnorm(0.95, mean = 100, sd = 10)
qnorm(0.95, mean = 100, sd = 10 / sqrt(50))
pbinom(4, size = 6, prob = 0.5, lower.tail = FALSE)
?pbinom
?ppois
lambda <- 0.2
n <- 40
sim <- 1:1000
set.seed(820)
means <- data.frame(x = sapply(nsims, function(x) {mean(rexp(n, lambda))}))
'''{r}
#First run the simulation
lambda <- 0.2
n <- 40
sim <- 1:1000
set.seed(820)
means <- data.frame(x = sapply(sim, function(x) {mean(rexp(n, lambda))}))
lambda <- 0.2
n <- 40
sim <- 1:1000
set.seed(820)
m <- data.frame(x = sapply(sim, function(x) {mean(rexp(n, lambda))}))
ambda <- 0.2
n <- 40
sim <- 1:1000
set.seed(820)
m <- data.frame(x = sapply(sim, function(x) {mean(rexp(n, lambda))}))
Print(m)
lambda <- 0.2
n <- 40
sim <- 1:1000
set.seed(820)
m <- data.frame(x = sapply(sim, function(x) {mean(rexp(n, lambda))}))
Print(m)
lambda <- 0.2
n <- 40
sim <- 1:1000
set.seed(820)
m <- data.frame(x = sapply(sim, function(x) {mean(rexp(n, lambda))}))
print(m)
summary(m)
lambda <- 0.2
n <- 40
sim <- 1:1000
set.seed(820)
m <- data.frame(x = sapply(sim, function(x) {mean(rexp(n, lambda))}))
mean(m$x)
m <- data.frame(x = sapply(sim, function(x) {mean(rexp(n, lambda))}))
lambda <- 0.2
n <- 40
sim <- 1:1000
set.seed(820)
m <- data.frame(x = sapply(sim, function(x) {mean(rexp(n, lambda))}))
m
mean(m$x)
sd(m$x)
library(ggplot2)
ggplot(data = m, aes(x = x)) +
geom_histogram(aes(y=..density..), fill = I('#00e6fa'),
binwidth = 0.20, color = I('black')) +
stat_function(fun = dnorm, arg = list(mean = 5, sd = sd(m$x)))
library(ggplot2)
ggplot(data = m, aes(x = x)) +
geom_histogram(aes(y=..density..), fill = I('#00565d'),
binwidth = 0.20, color = I('black')) +
stat_function(fun = dnorm, arg = list(mean = 5, sd = sd(m$x)))
data(ToothGrowth
data(ToothGrowth)
head(ToothGrowth)
N = 9
mean = 1100
sd = 30
error = qt(0.975, df = N-1) * sd / sqrt(N)
ans = mean + c(-1,1) * error
round(ans)
N = 9
mean = -2
# error = qt(0.95,df = 9)*sd/sqrt(N-1) = 2
ans = 2 * sqrt(N) / qt(0.975, df = N-1)
round(ans, 2)
nx = 10
ny = 10
mx = 3
my = 5
Sx2 = 0.6
Sy2 = 0.68
Sr = sqrt(((nx-1)*Sx2+(ny-1)*Sy2)/(nx+ny-2))
ans = mx - my + c(-1,1) * qt(0.975, df = nx + ny - 2) * Sr * sqrt(1/nx+1/ny)
round(ans, 2)
nx = 100
ny = 100
mx = 4
my = 6
Sx2 = 0.5
Sy2 = 2
Sr = sqrt(((nx-1)*Sx2+(ny-1)*Sy2)/(nx+ny-2))
ans = my - mx + c(-1,1) * qt(0.975, df = nx + ny - 2) * Sr * sqrt(1/nx+1/ny)
round(ans, 2)
nx = 10
ny = 10
mx = 3
my = 5
Sx2 = 0.6
Sy2 = 0.68
Sr = sqrt(((nx-1)*Sx2+(ny-1)*Sy2)/(nx+ny-2))
ans = mx - my + c(-1,1) * qt(0.975, df = nx + ny - 2) * Sr * sqrt(1/nx+1/ny)
round(ans, 2)
help(mtcars)
str(mtcars)
summary(mtcars)
ls()
attach(mtcars)
newdata <- mtcars[order(mpg),]
print(newdata)
sapply(mydata, mean. na.rm=TRUE)
sapply(newdata, mean. na.rm=TRUE)
sapply(mtcars)
sapply(mtcars$mpg, mean, na.rm=TRUE)
cor(mtcars, use="complete.obs", method="kendall")
cov(mtcars, use="complete.obs")
x <- mtcars[1:3]
y <- mtcars[4:6]
cor(x, y)
splom(mtcars)
data(mtcars)
names(mtcars)
mtcars$am <- as.factor(mtcars$am)
levels(mtcars$am) <- c("Automatic", "Manual")
boxplot(mpg~am, data = mtcars,
col = c("red", "green"),
xlab = "Type of Transmission",
ylab = "MPG",
main = "MPG by Type of Transmission")
data(mtcars)
names(mtcars)
mtcars$am <- as.factor(mtcars$am)
levels(mtcars$am) <- c("Automatic", "Manual")
boxplot(mpg~am, data = mtcars,
col = c("red", "green"),
xlab = "Type of Transmission",
ylab = "MPG",
main = "MPG: Automatic vs Manual")
levels(mtcars$am) <- c("Automatic", "Manual")
levels(mtcars$am)
summary(mtcars)
data(mtcars)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl) + wt + interaction(cyl, wt), data = mtcars)
# To compare model we usually use an anova table
# anova null hypothesis says that both models are the same.
compare <- anova(fit1, fit2)
compare$Pr
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl) + wt + interaction(cyl, wt), data = mtcars)
compare <- anova(fit1, fit2)
compare$Pr
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
# Give the hat diagonal for the most influential point
fit <- lm(y ~ x)
hatvalues(fit)
# Consider the following data set
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
# Give the slope dfbeta for the point with the highest hat value.
fit <- lm(y ~ x)
dfbetas(fit)
library(MASS)
data(shuttle)
# convert outcome to 0 = noauto, 1 = auto
shuttle$use <- factor(shuttle$use, levels = c("auto", "noauto"), labels = c(1, 0))
fit1 <- glm(use ~ wind - 1, data = shuttle, family = "binomial")
summary(fit)
windhead <- fit1$coef[1]
windtail <- fit1$coef[2]
exp(windtail)/exp(windhead)
library(MASS)
data(shuttle)
# convert outcome to 0 = noauto, 1 = auto
shuttle$use <- factor(shuttle$use, levels = c("auto", "noauto"), labels = c(1, 0))
# Question 2
# Consider the previous problem. Give the estimated odds ratio for autoloader
# use comparing head winds (numerator) to tail winds (denominator) adjusting for
# wind strength from the variable magn.
fit2 <- glm(use ~ wind + magn - 1, data = shuttle, family = "binomial")
summary(fit)
windhead2 <- fit2$coef[1]
windtail2 <- fit2$coef[2]
exp(windtail2)/exp(windhead2)
library(MASS)
data(shuttle)
# convert outcome to 0 = noauto, 1 = auto
shuttle$use <- factor(shuttle$use, levels = c("auto", "noauto"), labels = c(1, 0))
# Question 2
# Consider the previous problem. Give the estimated odds ratio for autoloader
# use comparing head winds (numerator) to tail winds (denominator) adjusting for
# wind strength from the variable magn.
fit2 <- glm(use ~ wind + magn - 1, data = shuttle, family = "binomial")
summary(fit)
windhead2 <- fit2$coef[1]
windtail2 <- fit2$coef[2]
exp(windtail2)/exp(windhead2)
data(mtcars)
head(mtcars)
summary(mtcars)
browse.workspace
help(read.csv)
ibrary(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# Fit a random forest predictor relating the factor variable y to the remaining variables.
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
b <- varImp(a)
order(b)
library(ElemStatLearn)
library(ElemStatLearn)
library(randomForest)
library(e1071)
library(medley)
install.packages(c("ElemStatLearn", "randomForest", "e1071", "medley"))
library(ElemStatLearn)
library(randomForest)
library(e1071)
library(medley)
data(vowel.train)
data(vowel.test)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
testError <- function(pred, data, outcome){
sum(predict(pred, data)!= outcome)/length(outcome)
}
#Random forest
vowelTree <- randomForest(y ~ . , data=vowel.train, prox = TRUE)
print("Test error random forest:")
vowelTreeError <- testErr
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
M <- train(y ~ ., data=vowel.train, method="rf")
varImp(M)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
M <- train(y ~ ., data=vowel.train, method="rf")
varImp(M)
vowel = rbind(vowel.test,vowel.train)
vowel$y = factor(vowel$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
install.packages("caret")
vowel = rbind(vowel.test,vowel.train)
vowel$y = factor(vowel$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# Fit a random forest predictor relating the factor variable y to the remaining variables.
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
b <- varImp(a)
order(b)
library(caret)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# Fit a random forest predictor relating the factor variable y to the remaining variables.
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
b <- varImp(a)
order(b)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
M <- train(y ~ ., data=vowel.train, method="rf")
varImp(M)
vowel = rbind(vowel.test,vowel.train)
vowel$y = factor(vowel$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
install.packages("ISLR")
library(ISLR)
library(ggplot2)
library(caret)
data(wage)
summary(wage)
library(ISLR)
library(ggplot2)
library(caret)
data(Wage)
summary(Wage)
inTrain <- createDataPartition(y=Wage$wage,
p=0.7, list=false)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(trainig);dim(testing)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training);dim(testing)
library(ISLR)
library(ggplot2)
library(caret)
inTrain <- createDataPartition(y=Wage$wage,
p=0.7, list=false)
inTrain <- createDataPartition(y=Wage$wage,
p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training);dim(testing)
featurePlot(x=training[,c("age","education","jobclass")],
y = training$wage,
plot="pairs")
qq <- qplot(age,wage,colour=education,data=training)
qq +  geom_smooth(method='lm',formula=y~x)
qplot(wage,colour=education,data=training,geom="density"
qplot(wage,colour=education,data=training,geom="density")
qplot(wage,colour=education,data=training,geom="density")
install.packages("Hmisc")
ibrary(caret)
library(knitr)
library(doMC)
registerDoMC(cores = 4)
set.seed(140819)
dat.train <- read.csv("pml-training.csv", stringsAsFactors=FALSE)
dat.test <- read.csv("pml-testing.csv", stringsAsFactors=FALSE)
setwd("C:/Users/owner/Desktop/R Program/Machine Learning")
library(caret)
library(knitr)
library(doMC)
registerDoMC(cores = 4)
set.seed(140819)
dat.train <- read.csv("pml-training.csv", stringsAsFactors=FALSE)
dat.test <- read.csv("pml-testing.csv", stringsAsFactors=FALSE)
# Function to filter the features
# Here, we just remove the features with any missing data
filterData <- function(idf) {
# Since we have lots of variables, remove any with NA's
# or have empty strings
idx.keep <- !sapply(idf, function(x) any(is.na(x)))
idf <- idf[, idx.keep]
idx.keep <- !sapply(idf, function(x) any(x==""))
idf <- idf[, idx.keep]
# Remove the columns that aren't the predictor variables
col.rm <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2",
"cvtd_timestamp", "new_window", "num_window")
idx.rm <- which(colnames(idf) %in% col.rm)
idf <- idf[, -idx.rm]
return(idf)
}
dat.train <- filterData(dat.train)
dat.train$classe <- factor(dat.train$classe)
dat.test <- filterData(dat.test)
cvCtrl <- trainControl(method = "cv", number = 5, allowParallel = TRUE, verboseIter = TRUE)
# We'll make 3 models that use different approaches and use a voting mechanism for the class predictions
m1 <- train(classe ~ ., data = dat.train, method = "rf", trControl = cvCtrl)
m2 <- train(classe ~ ., data = dat.train, method = "svmRadial", trControl = cvCtrl)
m3 <- train(classe ~ ., data = dat.train, method = "knn", trControl = cvCtrl)
# Make a data frame with the maximum accuracy values from the models obtained
# via the cross validation on the training data
acc.tab <- data.frame(Model=c("Random Forest", "SVM (radial)", "KNN"),
Accuracy=c(round(max(head(m1$results)$Accuracy), 3),
round(max(head(m2$results)$Accuracy), 3),
round(max(head(m3$results)$Accuracy), 3)))
kable(acc.tab)
test.pred.1 <- predict(m1, dat.test)
test.pred.2 <- predict(m2, dat.test)
test.pred.3 <- predict(m3, dat.test)
# Make a table and check if they all agree
pred.df <- data.frame(rf.pred = test.pred.1, svm.pred = test.pred.2, knn.pred = test.pred.3)
pred.df$agree <- with(pred.df, rf.pred == svm.pred && rf.pred == knn.pred)
all.agree <- all(pred.df$agree)
colnames(pred.df) <- c("Random Forest", "SVM", "KNN", "All Agree?")
kable(pred.df)
# Looks like they all do; let's write out the prediction files to submit
# This uses the code supplied by the class instructions
answers <- pred.df$rf.pred
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(answers)
answers
answers <- pred.df
answers
answers <- pred.df$SVM
answers
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(answers)
